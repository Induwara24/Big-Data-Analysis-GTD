{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b6cb1f-9694-47d5-b869-57a44c1a3621",
   "metadata": {},
   "source": [
    "# Data Preparation & Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d72548-4cdb-4524-931f-6a4778df5932",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea991eb-5727-4da2-976a-5f2a9be03b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        eventid  iyear  imonth  iday  approxdate  extended resolution  \\\n",
      "0  202101010004   2021       1     1         NaN         0        NaT   \n",
      "1  202101010005   2021       1     1  01/01/2021         0        NaT   \n",
      "2  202101010006   2021       1     1         NaN         0        NaT   \n",
      "3  202101010009   2021       1     1         NaN         0        NaT   \n",
      "4  202101010024   2021       1     2         NaN         0        NaT   \n",
      "\n",
      "   country  country_txt  region  ...  \\\n",
      "0       60        Egypt      10  ...   \n",
      "1       92        India       6  ...   \n",
      "2      228        Yemen      10  ...   \n",
      "3        4  Afghanistan       6  ...   \n",
      "4      182      Somalia      11  ...   \n",
      "\n",
      "                                            addnotes  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2  There is doubt that this incident meets terror...   \n",
      "3  There is doubt that this incident meets terror...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                              scite1  \\\n",
      "0  \"Egyptian officials: Roadside bombing in Sinai...   \n",
      "1  \"Terror module busted, seven arrested in J&K,\"...   \n",
      "2  \"Civilian reportedly dies of injuries from Hou...   \n",
      "3  \"Afghanistan- Ghor Provincial Council Member K...   \n",
      "4  \"Jubbaland lawmaker killed in car bomb blast,\"...   \n",
      "\n",
      "                                              scite2  \\\n",
      "0                                                NaN   \n",
      "1  \"Militants throw grenade on CRPF,\" HT Syndicat...   \n",
      "2  \"UN troubled by civilian deaths in Yemen's Hud...   \n",
      "3  \"Journalist killed on 1st day of new year in A...   \n",
      "4  \"Somali lawmaker killed in a sticky bomb attac...   \n",
      "\n",
      "                                              scite3  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2  \"Five women killed in Yemen wedding attack,\" G...   \n",
      "3  \"Afghanistan: 2021 Begins with Assassination o...   \n",
      "4  \"Al-Shabab claims killing regional MP in south...   \n",
      "\n",
      "                   dbsource  INT_LOG  INT_IDEO INT_MISC INT_ANY  related  \n",
      "0  START Primary Collection       -9        -9        0      -9      NaN  \n",
      "1  START Primary Collection       -9        -9        0      -9      NaN  \n",
      "2  START Primary Collection        0         0        0       0      NaN  \n",
      "3  START Primary Collection       -9        -9        0      -9      NaN  \n",
      "4  START Primary Collection        0         0        0       0      NaN  \n",
      "\n",
      "[5 rows x 135 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "df1 = pd.read_excel(\"globalterrorismdb_0522dist.xlsx\")\n",
    "df2 = pd.read_excel(\"globalterrorismdb_2021Jan-June_1222dist.xlsx\")\n",
    "\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1e7c7c-3afd-400a-bb2b-8ffd848d62f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        eventid  iyear  imonth  iday approxdate  extended resolution  country  \\\n",
      "0  197000000001   1970       7     2        NaN         0        NaT       58   \n",
      "1  197000000002   1970       0     0        NaN         0        NaT      130   \n",
      "2  197001000001   1970       1     0        NaN         0        NaT      160   \n",
      "3  197001000002   1970       1     0        NaN         0        NaT       78   \n",
      "4  197001000003   1970       1     0        NaN         0        NaT      101   \n",
      "\n",
      "          country_txt  region  ... addnotes scite1 scite2  scite3  dbsource  \\\n",
      "0  Dominican Republic       2  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "1              Mexico       1  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "2         Philippines       5  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "3              Greece       8  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "4               Japan       4  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "\n",
      "   INT_LOG  INT_IDEO INT_MISC INT_ANY  related  \n",
      "0        0         0        0       0      NaN  \n",
      "1        0         1        1       1      NaN  \n",
      "2       -9        -9        1       1      NaN  \n",
      "3       -9        -9        1       1      NaN  \n",
      "4       -9        -9        1       1      NaN  \n",
      "\n",
      "[5 rows x 135 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4423f-f785-48d9-b034-153a5a7a0cb8",
   "metadata": {},
   "source": [
    "## Convert the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef24007-96a6-4793-beba-c5c65ec6de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "df1.to_csv(\"globalterrorismdb_1970_2020.csv\", index=False)\n",
    "df2.to_csv(\"globalterrorismdb_2021.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ebf43e3-31c7-433c-975b-655958d0f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'approxdate' column to a string\n",
    "df1['approxdate'] = df1['approxdate'].astype(str)\n",
    "df2['approxdate'] = df2['approxdate'].astype(str)\n",
    "\n",
    "# Save as Parquet (recommended for Spark)\n",
    "df1.to_parquet(\"globalterrorismdb_1970_2020.parquet\", index=False)\n",
    "df2.to_parquet(\"globalterrorismdb_2021.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1eee5d-105e-4617-a0bf-68d4b31770cf",
   "metadata": {},
   "source": [
    "# Data Storage Using MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "167a698a-4edc-4f55-aaef-49c347c17bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sithu\\AppData\\Local\\Temp\\ipykernel_35660\\2524730832.py:10: DtypeWarning: Columns (4,31,33,54,61,62,63,76,79,90,92,94,96,114,115,121) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(\"globalterrorismdb_1970_2020.csv\")\n",
      "C:\\Users\\sithu\\AppData\\Local\\Temp\\ipykernel_35660\\2524730832.py:11: DtypeWarning: Columns (79,114,115) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(\"globalterrorismdb_2021.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"GlobalTerrorism\"]\n",
    "collection = db[\"GTD\"]\n",
    "\n",
    "# Load CSV\n",
    "df1 = pd.read_csv(\"globalterrorismdb_1970_2020.csv\")\n",
    "df2 = pd.read_csv(\"globalterrorismdb_2021.csv\")\n",
    "\n",
    "# Convert NaN to None for MongoDB compatibility\n",
    "df1 = df1.where(pd.notna(df1), None)\n",
    "df2 = df2.where(pd.notna(df2), None)\n",
    "\n",
    "# Insert into MongoDB\n",
    "collection.insert_many(df1.to_dict(orient=\"records\"))\n",
    "collection.insert_many(df2.to_dict(orient=\"records\"))\n",
    "\n",
    "print(\"CSV Data inserted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513c0ee-f8ec-48f7-babd-b8a282a9453f",
   "metadata": {},
   "source": [
    "## Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc11a92b-96b2-42c5-9aa6-98fc18c777cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('67accb675a2fd9c0c2e5e739'), 'eventid': 197000000001, 'iyear': 1970, 'imonth': 7, 'iday': 2, 'approxdate': None, 'extended': 0, 'resolution': None, 'country': 58, 'country_txt': 'Dominican Republic', 'region': 2, 'region_txt': 'Central America & Caribbean', 'provstate': 'National', 'city': 'Santo Domingo', 'latitude': 18.456792, 'longitude': -69.951164, 'specificity': 1.0, 'vicinity': 0, 'location': None, 'summary': None, 'crit1': 1, 'crit2': 1, 'crit3': 1, 'doubtterr': 0, 'alternative': nan, 'alternative_txt': None, 'multiple': 0.0, 'success': 1, 'suicide': 0, 'attacktype1': 1, 'attacktype1_txt': 'Assassination', 'attacktype2': nan, 'attacktype2_txt': None, 'attacktype3': nan, 'attacktype3_txt': None, 'targtype1': 14, 'targtype1_txt': 'Private Citizens & Property', 'targsubtype1': 68.0, 'targsubtype1_txt': 'Named Civilian', 'corp1': None, 'target1': 'Julio Guzman', 'natlty1': 58.0, 'natlty1_txt': 'Dominican Republic', 'targtype2': nan, 'targtype2_txt': None, 'targsubtype2': nan, 'targsubtype2_txt': None, 'corp2': None, 'target2': None, 'natlty2': nan, 'natlty2_txt': None, 'targtype3': nan, 'targtype3_txt': None, 'targsubtype3': nan, 'targsubtype3_txt': None, 'corp3': None, 'target3': None, 'natlty3': nan, 'natlty3_txt': None, 'gname': 'MANO-D', 'gsubname': None, 'gname2': None, 'gsubname2': None, 'gname3': None, 'gsubname3': None, 'motive': None, 'guncertain1': 0.0, 'guncertain2': nan, 'guncertain3': nan, 'individual': 0, 'nperps': nan, 'nperpcap': nan, 'claimed': nan, 'claimmode': nan, 'claimmode_txt': None, 'claim2': nan, 'claimmode2': nan, 'claimmode2_txt': None, 'claim3': nan, 'claimmode3': nan, 'claimmode3_txt': None, 'compclaim': nan, 'weaptype1': 13, 'weaptype1_txt': 'Unknown', 'weapsubtype1': nan, 'weapsubtype1_txt': None, 'weaptype2': nan, 'weaptype2_txt': None, 'weapsubtype2': nan, 'weapsubtype2_txt': None, 'weaptype3': nan, 'weaptype3_txt': None, 'weapsubtype3': nan, 'weapsubtype3_txt': None, 'weaptype4': nan, 'weaptype4_txt': None, 'weapsubtype4': nan, 'weapsubtype4_txt': None, 'weapdetail': None, 'nkill': 1.0, 'nkillus': nan, 'nkillter': nan, 'nwound': 0.0, 'nwoundus': nan, 'nwoundte': nan, 'property': 0, 'propextent': nan, 'propextent_txt': None, 'propvalue': nan, 'propcomment': None, 'ishostkid': 0.0, 'nhostkid': nan, 'nhostkidus': nan, 'nhours': nan, 'ndays': nan, 'divert': None, 'kidhijcountry': None, 'ransom': 0.0, 'ransomamt': nan, 'ransomamtus': nan, 'ransompaid': nan, 'ransompaidus': nan, 'ransomnote': None, 'hostkidoutcome': nan, 'hostkidoutcome_txt': None, 'nreleased': nan, 'addnotes': None, 'scite1': None, 'scite2': None, 'scite3': None, 'dbsource': 'PGIS', 'INT_LOG': 0, 'INT_IDEO': 0, 'INT_MISC': 0, 'INT_ANY': 0, 'related': None}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve data\n",
    "document = collection.find_one()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7307bde-8892-428c-9dce-6ad11e23be97",
   "metadata": {},
   "source": [
    "# Data Processing Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b00c654-6153-4184-aaf1-9c3193b04ed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@ROG_FLOW_X13:57102\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize Spark Session with MongoDB Connector\u001b[39;00m\n\u001b[0;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGTD Analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.mongodb.input.uri\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongodb://localhost:27017/GlobalTerrorism.GTD\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.mongodb.output.uri\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongodb://localhost:27017/GlobalTerrorism.GTD\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[0;32m    206\u001b[0m         sparkHome,\n\u001b[0;32m    207\u001b[0m         pyFiles,\n\u001b[0;32m    208\u001b[0m         environment,\n\u001b[0;32m    209\u001b[0m         batchSize,\n\u001b[0;32m    210\u001b[0m         serializer,\n\u001b[0;32m    211\u001b[0m         conf,\n\u001b[0;32m    212\u001b[0m         jsc,\n\u001b[0;32m    213\u001b[0m         profiler_cls,\n\u001b[0;32m    214\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39m_jconf)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mJavaSparkContext(jconf)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@ROG_FLOW_X13:57102\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session with MongoDB Connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GTD Analysis\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/GlobalTerrorism.GTD\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/GlobalTerrorism.GTD\") \\\n",
    "    .getOrCreate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
